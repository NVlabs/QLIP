<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="Video Language Model, Video Instruction Tuning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title></title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Q-LIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation</h1>
          <h1 class="title is-4 publication-conference"></h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="http://zhaoyue-zephyrus.github.io" target="_blank">Yue Zhao</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://xuefuzhao.github.io/" target="_blank">Fuzhao Xue</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://reedscot.github.io/" target="_blank">Scott Reed</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://jimfan.me/" target="_blank">Linxi "Jim" Fan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://yukezhu.me/" target="_blank">Yuke Zhu</a><sup>1</sup>,
            </span>
            <span class="author-block">
                <a href="https://jankautz.com/" target="_blank">Jan Kautz</a><sup>1</sup>,
            </span>
            <span class="author-block">
                <a href="https://chrisding.github.io/" target="_blank">Zhiding Yu</a><sup>1</sup>,
            </span>
            <span class="author-block">
                <a href="http://philkr.net" target="_blank">Philipp Kr&auml;henb&uuml;hl</a><sup>2</sup>,
            </span>
            <span class="author-block">
                <a href="https://ai.stanford.edu/~dahuang/" target="_blank">De-An Huang</a><sup>1</sup>
            </span>
        </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>NVIDIA Research</span>
            <span class="author-block"><sup>2</sup>UT Austin</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2501.yyyyy"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- HF Link. -->
              <span class="link-block">
                <a href="https://github.com/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    ðŸ¤—
                  </span>
                  <span>HF Models</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
            <p>
            We introduce Quantized Language-Image Pretraining <b>(Q-LIP)</b>,
            a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding.
            Q-LIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives.
            We are the first to show that the two objectives do not need to be at odds.
            We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective.
            We validate the effectiveness of Q-LIP for multimodal understanding and text-conditioned image generation with a single model.
            Specifically, Q-LIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance.
            Finally, we demonstrate that Q-LIP enables a unified mixed-modality auto-regressive model for understanding and generation.
            </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
      <!-- Overview. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
              <p>
                State-of-the-art visual tokenizers excel at either understanding (high zero-shot accuracy, e.g. SigLIP) or reconstruction (low reconstruction FID, e.g. RQ-VAE), but not both.
                Q-LIP can perform well on both understanding and reconstruction with a marginal performance drop, opening up an opportunity for unified multi-modal understanding and generation.
              </p>
              <div class="content has-text-justified">
                <img src="./static/images//overview.png" class="center" alt="method overview">
              </div>
        </div>
        </div>
      </div>
      <!--/ Overview. -->
  
    </div>
  </section>

<section class="section">
    <div class="container is-max-desktop">

        <div class="columns is-centered">
    
          <!-- Method Overview. -->
          <div class="column">
            <div class="content">
              <h2 class="title is-3">Training Q-LIP in two stages</h2>
              <div class="content has-text-justified">
                <img src="./static/images/training.png" width="60%" class="center" alt="training">
              </div>
              <p>
                Stage (1):  we optimize a weighted sum of reconstruction loss, quantization loss, and contrastive loss <i>without</i> the perceptual and adversarial loss. 
              </p>
              <p>
                Stage (2): we improve the reconstruction quality and restore higher-frequency details by fine-tuning the quantization bottleneck and the visual decoder.
                We drop the text encoder and freeze the visual encoder to prevent degradation when the batch-size restriction is relaxed.
              </p>
              <div class="content has-text-justified">
                <img src="./static/images/recon.png" width="60%" class="center" alt="reconstruction">
              </div>
            </div>
          </div>
          <!--/ Method Overview. -->

        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
  
      <div class="columns is-centered">
  
        <div class="column">
          <div class="content">
            <h2 class="title is-3">Evaluating Q-LIP</h2>
            <p>
              We conduct a linear probing evaluation to compare all visual encoder methods.
              The methods include (1) reconstruction-only tokenizers such as VQ-VAE and BSQ-ViT, (2) language-quantized tokenizers, such as LQAE,
              and (3) CLIP-style vision encoder (without decoder), such as EVA-CLIP.
              We see significant improvement in linear probing classsification accuracy over reconstruction-only tokenizers and language-quantized tokenizers.
              In addition, Q-LIP is very close to EVA-CLIP.
            </p>
            <div class="content has-text-centered">
              <img src="./static/images/linear_probe.png" alt="q-lip linear probe">
            </div>
        </div>
        </div>
    </div>
</section>
  
<section class="section">
    <div class="container is-max-desktop">
  
      <div class="columns is-centered">

        <div class="column">
          <div class="content">
            <h2 class="title is-3">Q-LIP For Text-to-Image Generation</h2>
              <p>
                We show the generated images by LlamaGen with its original VQGAN (<b>left</b>) and our Q-LIP (<b>right</b>) side by side with the same caption in the bottom.
                We can see images generated by Q-LIP follow the captions better by depicting all aspects that might be missing from the baseline with VQ-GAN.
              </p>
              <div class="content has-text-centered">
                <img src="./static/images/llamagen.png" width="100%" alt="llamagen">
              </div>
            </div>
  
          </div>
        </div>
      </div>
    </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhao2024viit,
  author    = {Zhao, Yue and Xue, Fuzhao and Reed, Scott and Fan, Linxi and Zhu, Yuke and Kautz, Jan and Yu, Zhiding and Kr&auml;henb&uuml;hl, Philipp and Huang, De-An},
  title     = {Q-LIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation},
  journal   = {arXiv preprint arXiv:2501.yyyyy},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/contents/viit_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/zhaoyue-zephyrus" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The website template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies</a>.
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
